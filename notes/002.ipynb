{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.10.19","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d920a0f9-53c3-48b1-b550-f7ce509b3f45","cell_type":"markdown","source":"# Multivariate Normal\n\n## Things To Note\n### Notation\n$ Y_i = \\begin{pmatrix} {Y}_{i1} \\\\\\\\ ⋮ \\\\\\\\ {Y}_{in_i} \\\\\\\\ \\end{pmatrix}  $\n\n### Assumptions\nFirstly, subject $i$ data is independent of subject $j$ data, $Y_i \\perp\\!\\!\\!\\perp Y_j$, $i \\ne j$.  \nSecondly, for subject $i$, their data at $k$ is not independent of their data at $k'$, $Y_{ik} \\not\\!\\perp\\!\\!\\!\\perp Y_{ik'}$, $k \\ne k'$.\n\n### Reminder of Transformations\nLet $a,b,c,d$ be constants. Also, $ Y_i = \\begin{pmatrix} {Y}_{i1} \\\\\\\\ ⋮ \\\\\\\\ {Y}_{in_i} \\\\\\\\ \\end{pmatrix}  $.  \n1. $E[aY+b] = aE[Y]+b$\n2. $\\text{Cov}(aY_1 + b, cY_2 +d)=ac*\\text{Cov}(Y_1,Y_2)$\n3. Let $c=(c_1,...,c_n)^T$. $E[c^TY]=c^TE[Y]$.\n4. $\\text{Var}(c^TY)=c^T\\text{Var}(Y)c$\n\n## Multivariate Normal (MVN)\n$Y \\sim MVN(\\mu, \\Sigma)$ where $\\mu$ is the mean, which helps us capture the average patterns over time, and $\\Sigma$ is the covariance matrix, which helps us model dependency/correlations.   \nIn this class, we will learn techniques to model the mean $\\mu$ so that we represent the average pattern in the repeated measures over time, and model the covariance $\\Sigma$ so that we best represent the correlated dependencies in the repeated measures over time.  \n-  $\\mu_{n \\times 1} = E[Y_{n \\times 1}]$\n-  $\\Sigma_{n \\times n} = \\text{Cov}(Y_{n \\times 1}) = E{(Y-\\mu)(Y-\\mu)^T} = \\begin{pmatrix} \\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1n} \\\\ \\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\sigma_{n1} & \\sigma_{n2} & \\cdots & \\sigma_{nn} \\end{pmatrix}$ where the variances are on the diagonal and the covariances are on the off diagonal. When we model $\\Sigma$, the covariance matrix, we have to make sure our model ensures $\\Sigma$ is symmetric and positive definite.\n- $R_{n \\times n} = \\text{Corr}(Y_{n \\times 1}) = \\begin{pmatrix} 1 & \\rho_{12} & \\cdots & \\rho_{1n} \\\\ \\rho_{21} & 1 & \\cdots & \\rho_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\rho_{n1} & \\rho_{n2} & \\cdots & 1 \\end{pmatrix}$, where $\\rho_{jk} = \\sigma_{jk} / \\sqrt{\\sigma_{jj} \\sigma_{kk}} \\in [-1,1]$, which is the correlation between the $j$th and $k$th element of $Y= ({Y}_{1}, \\cdots, {Y}_{j}, \\cdots, {Y}_{k}, \\cdots, {Y}_{n})^T$.","metadata":{}},{"id":"66626a35-3c11-46ce-9f66-d9fc90a9d1fe","cell_type":"markdown","source":"### Definition Reminders\n#### Positive Definite\n$\\Sigma_{n \\times n}$ is **positive definite** if for any vector $c \\ne 0$, $c^T\\Sigma c > 0$. **Note**: If $\\Sigma_{n \\times n}$ is positive definite, then $\\Sigma_{n \\times n}$ is full rank.  \nA matrix is **full rank** if there is no $c_0 \\ne 0$ such that $c_0^TY=k$, where $k$ is a constant.  \n##### Example\nSuppose $Y_{3 \\times 1}$ and $\\text{Cov}(Y_{3 \\times 1})=\\Sigma_{3 \\times 3} \\equiv I_{3 \\times 3}$, which is the identity matrix.  \nTake any $c=(c_1,c_2,c_3)^T \\ne 0$.  \n$c^T\\Sigma c = \\begin{pmatrix} c_1 & c_2 & c_3 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{pmatrix} = \\begin{pmatrix} c_1 & c_2 & c_3 \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{pmatrix} = c_1^2 + c_2^2 + c_3^2 > 0$.  \nTherefore, $I_{3 \\times 3}$ is positive definite.\n#### Positive Semi-definite\n$\\Sigma_{n \\times n}$ is **positive semi-definite** if for any vector $c \\ne 0$, $c^T\\Sigma c ≥ 0$, or if $c_0^TY=k$ for some$c_0 \\ne 0$ (meaning $\\Sigma$ is not full rank), or $\\text{Var}(c_0^TY)=0$.\n##### Example\nTake $Y_{3 \\times 1} = \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\end{pmatrix}$ and define this transformation $Z = \\begin{pmatrix} Y_1 - \\bar{Y} \\\\ Y_2 - \\bar{Y} \\\\ Y_3 - \\bar{Y} \\end{pmatrix}$ where $\\bar{Y} = \\frac{Y_1 + Y_2 + Y_3}{3}$.  \nWe will show $\\text{Cov}(Z)$ is positive semi-definite.   \nTake $c_0=(1/3,1/3,1/3)^T$.  \n$c_0^TZ= \\begin{pmatrix} 1/3 & 1/3 & 1/3 \\end{pmatrix} \\begin{pmatrix} Y_1 - \\bar{Y} \\\\ Y_2 - \\bar{Y} \\\\ Y_3 - \\bar{Y} \\end{pmatrix} = \\frac{1}{3}(Y_1+Y_2+Y_3)-\\frac{1}{3}(\\bar{Y}+\\bar{Y}+\\bar{Y})=\\bar{Y}-\\bar{Y}=0$. Our constant $k$ is $0$ here, so we showed $c_0^TZ=0$ ($c_0^TY=k$ from above).  \nNext, we compute $\\text{Var}(c_0^TZ)$.  \n$Z_i = Y_i - \\bar{Y} = Y_i - \\frac{1}{3}(Y_1+Y_2+Y_3)$  \n$Z = \\begin{pmatrix} Y_1 - \\bar{Y} \\\\ Y_2 - \\bar{Y} \\\\ Y_3 - \\bar{Y} \\end{pmatrix} = \\begin{pmatrix} 1-\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & 1-\\frac{1}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{1}{3} & 1-\\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\end{pmatrix} = AY$  \n$\\text{Var}(c_0^TZ) = c_0^T\\text{Var}(Z)c_0= \\begin{pmatrix} 1/3 & 1/3 & 1/3 \\end{pmatrix} \\text{Var}(AY) \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix} = \\begin{pmatrix} 1/3 & 1/3 & 1/3 \\end{pmatrix} A \\text{Var}(Y) A^T \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix}$\n$\\text{Var}(c_0^TZ) = \\begin{pmatrix} 1/3 & 1/3 & 1/3 \\end{pmatrix} \\begin{pmatrix} \\frac{2}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{2}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{2}{3} \\end{pmatrix} \\text{Var}(Y) \\begin{pmatrix} \\frac{2}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{2}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix}$  \nWe don't have to compute this full thing, just $c_0^TA = \\begin{pmatrix} 1/3 & 1/3 & 1/3 \\end{pmatrix} \\begin{pmatrix} \\frac{2}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{2}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{2}{3} \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 \\end{pmatrix}$. Therefore, $\\text{Var}(c_0^TZ)=0$.  \nPutting everything together, we have that $\\text{Cov}(Z)$ is positive semi-definite.","metadata":{}},{"id":"febcc9cb-3ba4-4f33-84fd-f444f081a564","cell_type":"markdown","source":"### Unbiased Estimators of $\\mu$ and $\\Sigma$\nLet $ Y_i = \\begin{pmatrix} {Y}_{i1} & \\cdots & {Y}_{in} \\end{pmatrix}^T$ and $Y_i \\sim MVN(\\mu, \\Sigma)$.\n#### Sample Mean \nAn unbiased estimator of $\\mu$ is the sample mean, $\\bar{Y} = \\frac{1}{N} \\sum_{i=1}^{N} Y_i$. $\\bar{Y}$ is an unbiased estimator because $E[\\bar{Y}]=\\mu$. \n##### Proof of $E[\\bar{Y}]=\\mu$\n$E[\\bar{Y}] = E[\\frac{1}{N} \\sum_{i=1}^{N} Y_i] = \\frac{1}{N}(N\\mu) = \\mu$  \n$\\text{Cov}(\\bar{Y}) = \\text{Cov}(\\frac{1}{N} \\sum_{i=1}^{N} Y_i) = \\frac{1}{N^2}\\text{Cov}(\\sum_{i=1}^{N} Y_i)=\\frac{1}{N^2}N\\text{Cov}(Y_1)=\\frac{1}{N}\\text{Cov}(Y_1)=\\frac{1}{N}\\Sigma$\n#### Sample Covariance\nAn unbiased estimator of $\\Sigma$ is the sample covariance, $S_{n \\times n} = \\frac{1}{N-1} \\sum_{i=1}^{N} (Y_i - \\bar{Y})(Y_i - \\bar{Y})^T$. $S_{n \\times n}$ is an unbiased estimator because $E[S]=\\Sigma$. \nWhen we use the sample covariance $S$ as our estimator of $\\Sigma$, we are assuming $\\Sigma$ is unstructured. Unstructured means we are estimating every element of $\\Sigma = \\begin{pmatrix} \\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1n} \\\\ \\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\sigma_{n1} & \\sigma_{n2} & \\cdots & \\sigma_{nn} \\end{pmatrix}$. If I had assumed $\\Sigma$ was sturctured, that means I'm putting a structure in $\\Sigma$ to estimate fewer parameter.  \n**Examples of Structured $\\Sigma$**  \n$\\Sigma = \\text{diag}(\\sigma_{11}, \\cdots, \\sigma_{nn})$  \n$\\Sigma = \\begin{pmatrix} \\sigma_{11} & \\sigma_{12} & \\cdots & 0 \\\\ \\sigma_{21} & \\sigma_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\sigma_{nn} \\end{pmatrix}$ (This assumes all $0$'s except the diagonal and the $1,2$ block.)\n##### Proof of $E[S]=\\Sigma$\n$E[(Y_i - \\bar{Y})(Y_i - \\bar{Y})^T]=E[Y_i Y_i^T] + E[\\bar{Y} \\bar{Y}^T] - E[Y_i \\bar{Y}^T] - E[\\bar{Y} Y_i^T] = [\\Sigma + \\mu \\mu^T] + [\\frac{1}{N}\\Sigma + \\mu \\mu^T] - E[Y_i \\bar{Y}^T] - E[\\bar{Y} Y_i^T]$  \nThen, using $\\Sigma = E[Y_iY_i^T] - \\mu \\mu^T$.  \n$E[Y_i \\bar{Y}^T] = \\frac{1}{N}\\sum_{i=1}^{N}E[Y_i Y_k^T] = \\frac{1}{N}\\{\\Sigma+ \\mu \\mu^T + (N-1)\\mu \\mu^T \\}$  \nNote: $\\Sigma+ \\mu \\mu^T$ is for $i=k$ and $(N-1)\\mu \\mu^T$ is for $i \\ne k$.\nThus, $E[(Y_i - \\bar{Y})(Y_i - \\bar{Y})^T]= \\Sigma(1+ \\frac{1}{N}) + 2\\mu \\mu^T - \\frac{2}{N}\\Sigma - 2\\mu \\mu^T = \\Sigma(1 - \\frac{1}{N})$.  \nFinally, $E[S] = E[\\frac{1}{N-1} \\sum_{i=1}^{N}(Y_i - \\bar{Y})(Y_i - \\bar{Y})^T] = \\frac{1}{N-1} (N) \\frac{N-1}{N} \\Sigma = \\Sigma$\n","metadata":{}},{"id":"5eab487e-cf6d-489e-868e-a83763ea2c49","cell_type":"markdown","source":"## Transformations of MVNs\nAssume $Y \\sim MVN(\\mu, \\Sigma)$ and $a$ and $c$ are $r \\times n$ and $r \\times 1$ matrices of constants, respectively.  \nThe transformation $Z_{r \\times 1} = aY+c \\sim MVN(a\\mu + c, a\\Sigma a^T)$.\n\n## Conditional Distributions of MVNs\nAssume $Y \\sim MVN(\\mu, \\Sigma)$. $Y$ can be written as $Y = \\begin{pmatrix} Y_1{}_{\\,q\\times 1} \\\\ Y_2{}_{\\,r\\times 1} \\end{pmatrix}$, where $\\mu = \\begin{pmatrix} \\mu_1{}_{\\,q\\times 1} \\\\ \\mu_2{}_{\\,r\\times 1} \\end{pmatrix}$ and $\\Sigma = \\begin{pmatrix} \\Sigma_{11}{}_{\\,q\\times q} & \\Sigma_{12}{}_{\\,q\\times r} \\\\ \\Sigma_{21}{}_{\\,r\\times q} & \\Sigma_{22}{}_{\\,r\\times r} \\end{pmatrix}$.  \nThe conditional distribution of $Y_1$ given $Y_2 = y_2$, $Y_1 | Y_2 = y_2$, is $MVN(\\mu_{Y_1 | Y_2}, \\Sigma_{Y_1 | Y_2}$.   \n$\\mu_{Y_1 | Y_2} = E[Y_1 | Y_2 = y_2] = \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(y_2 - \\mu_2)$  \n$\\Sigma_{Y_1 | Y_2} = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}$","metadata":{}},{"id":"aa0ea63b-60d9-478c-b34c-1023fb3632f7","cell_type":"markdown","source":"## Example: Dental Study\nThis study had 27 participants, 16 boys and 11girls. The distance from the pituitary gland to pterygomaxillary fissure measured at ages 8, 10, 12, and 14. To note, a greater distance is better for orthodontic therapy.  \n![](images/002_1.png)","metadata":{}},{"id":"c3f0da20-846c-40d8-b565-1a349d101a26","cell_type":"markdown","source":"We will use the following data:   \n![](images/002_2.png)","metadata":{}},{"id":"302be646-ab87-4675-8df1-a3b1ce689fd1","cell_type":"markdown","source":"Let $Y_{i1}$ be the distance measure for girl $i$ at 8 years old, and $Y_{i2}$ be the distance measure for girl $i$ at 10 years old.  \n$Y_i = \\begin{pmatrix} Y_{i1} \\\\ Y_{i2} \\end{pmatrix} \\sim MVN\\left\\{\\begin{pmatrix} 21.18 \\\\ 22.23 \\end{pmatrix}, \\begin{pmatrix} 4.51 & 3.35 \\\\ 3.35 & 3.62 \\end{pmatrix} \\right\\}$  \nNote: We used the sample mean and sample covariance because they're unbiased estimators.  \n  \nNow suppose I want to compare distances at age 8 with the change in distance from ages 8 to 10 years.  \nLet $Y_i = \\begin{pmatrix} Z_{i1} \\\\ Z_{i2} \\end{pmatrix} = \\begin{pmatrix} Y_{i1} \\\\ Y_{i2} - Y_{i1} \\end{pmatrix}$.  \nReminder: $Z = aY + c \\sim MVN(a\\mu, a\\Sigma a^T)$.  \n$Z_i = \\begin{pmatrix} Y_{i1} \\\\ Y_{i2} - Y_{i1} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\end{pmatrix} \\begin{pmatrix} Y_{i1} \\\\ Y_{i2} \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$  \n\n$E[Z_i] = a\\mu + c = \\begin{pmatrix} 21.18 \\\\ 1.05 \\end{pmatrix}$  \n$\\text{Cov}(Z_i) = a\\Sigma a^T = \\begin{pmatrix} 4.51 & -1.16 \\\\ -1.16 & 1.42 \\end{pmatrix} $  \nFrom $\\text{Cov}(Y_i)$ and $\\text{Cov}(Z_i)$, we can compute the correlation matrices. We will get the following results: $\\text{Corr}(Y_{i1}, Y_{i2}) = 0.83$, which is high. Since the correlation is 0.83, the distance measure for the girl at age 10 is more likely to increase.  \n$\\text{Corr}(Z_{i1}, Z_{i2}) = -0.46$. This means that if the distance measure at age 8 is small (or large), the difference from 8 to 10 will be big (or small). ","metadata":{}}]}