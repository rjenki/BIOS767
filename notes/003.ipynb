{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.10.19","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"3b71a30c-40d7-4601-9966-2063812800a1","cell_type":"markdown","source":"# ML and REML Estimation Methods\n## Estimation of $\\mu$ and $\\Sigma$\n### Sample Mean and Sample Covariance\n$Y_i \\sim MVN(\\mu,\\Sigma)$, $i = 1,\\ldots,N$  \nOne type of estimator for $\\mu$ and $\\Sigma$ is the sample mean, $\\bar{Y}= \\frac{1}{N}\\sum_{i=1}^N Y_i$ & sample covariance, $S = \\frac{1}{N-1}\\sum_{i=1}^N (Y_i-\\bar{Y})(Y_i-\\bar{Y})^{T}$.   \n### Maximum Likelihood Estimation\nAnother type of estimator for $\\mu$ and $\\Sigma$ is maximum likelihood estimators. The likelihood is $\\mathcal{L}(\\mu,\\Sigma\\mid Y_1,\\ldots,Y_N)\n= \\frac{\\exp\\left\\{-\\frac12\\sum_{i=1}^N (Y_i-\\mu)^T\\Sigma^{-1}(Y_i-\\mu)\\right\\}\n}{(2\\pi)^{1/2\\,Nn}\\,|\\Sigma|^{N/2}}$.\nTo maximize this likelihood, we will use some tricks.\n\n#### Trick #1:\nSeparate the likelihood into 2 parts, one part that involves $\\mu$ another part that does not involve $\\mu$.\nTo do that trick, we write $Y_i-\\mu = Y_i-\\bar{Y} + \\bar{Y}-\\mu$.  \n$\\mathcal{L}(\\mu,\\Sigma\\mid Y_1,\\ldots,Y_N) =\n\\frac{\\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^N (Y_i-\\bar{Y})^T\\Sigma^{-1}(Y_i-\\bar{Y})\\right\\}}{(2\\pi)^{1/2\\,Nn}\\,|\\Sigma|^{N/2}}\\cdot\\exp\\left\\{-\\frac{1}{2} N(\\bar{Y}-\\mu)^T\\Sigma^{-1}(\\bar{Y}-\\mu)\\right\\}$","metadata":{}},{"id":"c61a82d5-4fdf-41b4-9bf2-543f9aebc604","cell_type":"markdown","source":"#### Trick #2:\nFor a scalar $b$, $\\text{tr}(b)=b$.  \n$\\mathcal{L}(\\mu,\\Sigma\\mid Y_1,\\ldots,Y_N) = \\frac{\\exp\\left\\{-\\frac{1}{2}\\,\\mathrm{tr}\\left(\\sum_{i=1}^N (Y_i-\\bar{Y})\\Sigma^{-1}(Y_i-\\bar{Y})^T\\right)\\right\\}}{(2\\pi)^{1/2\\,Nn}\\,|\\Sigma|^{N/2}}\n\\cdot\\exp\\left\\{-\\frac{1}{2} N(\\bar{Y}-\\mu)^T\\Sigma^{-1}(\\bar{Y}-\\mu)\\right\\}$  \n","metadata":{}},{"id":"fb184d1a-1b1b-466a-acce-175683020a55","cell_type":"markdown","source":"#### Trick #3: \n$\\mathrm{tr}(A)=\\mathrm{tr}(A^T)$  \n$\\mathcal{L}(\\mu,\\Sigma\\mid Y_1,\\ldots,Y_N) = \\frac{\\exp\\left\\{-\\frac{1}{2}\\,\\mathrm{tr}\\left(\\sum_{i=1}^N (Y_i-\\bar{Y})(Y_i-\\bar{Y})^T \\Sigma^{-1}\\right)\\right\\}}{(2\\pi)^{1/2\\,Nn}\\,|\\Sigma|^{N/2}}\n\\cdot\\exp\\left\\{-\\frac{1}{2} N(\\bar{Y}-\\mu)^T\\Sigma^{-1}(\\bar{Y}-\\mu)\\right\\}$  ","metadata":{}},{"id":"44f27165-b1a7-41f4-8c72-09fb6971cc22","cell_type":"markdown","source":"#### Trick #4:\n$S=\\frac{1}{N-1}\\sum_{i=1}^N (Y_i-\\bar{Y})(Y_i-\\bar{Y})^T$  \n$\\mathcal{L}(\\mu,\\Sigma\\mid Y_1,\\ldots,Y_N) = \\frac{\\exp\\left\\{-\\frac{1}{2}\\,\\mathrm{tr}\\left((N-1)S\\Sigma^{-1}\\right)\\right\\}\n}{ (2\\pi)^{1/2\\,Nn}\\,|\\Sigma|^{N/2}} \\cdot \\exp\\left\\{-\\frac12 N(\\bar{Y}-\\mu)^T\\Sigma^{-1}(\\bar{Y}-\\mu)\\right\\}$","metadata":{}},{"id":"c2c2f61d-19d6-4290-ad97-359d564d98d7","cell_type":"markdown","source":"#### Trick #5:\n$\\frac{1}{|A|}=|A^{-1}|$  \n$\\log \\mathcal{L}(\\mu,\\Sigma\\mid Y_1,\\ldots,Y_N)\n\\propto\n-\\frac{N}{2}\\log|\\Sigma|\n-\\frac{1}{2}\\,\\mathrm{tr}\\left((N-1)S\\Sigma^{-1}\\right)\n-\\frac{1}{2} N(\\bar{Y}-\\mu)^T\\Sigma^{-1}(\\bar{Y}-\\mu)\n$","metadata":{}},{"id":"6609b8fb-6c41-4a01-8cb6-7c2d27b43e0e","cell_type":"markdown","source":"### Maximize This Log-Likelihood w.r.t. $\\mu$\nWe only need to maximize the term w.r.t. $\\mu$, which is $-\\frac{1}{2} N(\\bar{Y}-\\mu)^T\\Sigma^{-1}(\\bar{Y}-\\mu)$.  \nTake derivative w.r.t. $\\mu$. To maximize this function, we want the value of $\\mu$ that makes this $0$.  \n$\\frac{\\partial}{\\partial\\mu} \\left[(\\bar{Y}-\\mu)^T\\Sigma^{-1}(\\bar{Y}-\\mu)\\right]= 0$  \nThus, $\\hat{\\mu}_{MLE} = \\bar{Y}$.","metadata":{}},{"id":"9bf610f4-6778-4dfe-aa60-61afac8f04d9","cell_type":"markdown","source":"### Maximize This Log-Likelihood w.r.t $\\Sigma$  \nNotes:  \n1. $\\frac{\\partial \\log|A|}{\\partial A} = A^{-1}$\n2. $\\frac{\\partial \\text{tr}(BA)}{\\partial A} = B$ if $A$ is symmetric.\n3. $\\frac{\\partial a^TXb}{\\partial X} = ba^T$\n\n$\\log \\mathcal{L}(\\Sigma) \\propto \\frac{N}{2}\\log|\\Sigma^{-1}| -\\frac{1}{2}\\,\\mathrm{tr}\\left((N-1)S\\Sigma^{-1}\\right) -\\frac{1}{2} N(\\bar{Y}-\\mu)^T\\Sigma^{-1}(\\bar{Y}-\\mu)$  \n  \nPlugging back $\\hat{\\mu}_{MLE} = \\bar{Y}$ into the log likelihood, we get: $log \\mathcal{L}(\\Sigma) \\propto \\frac{N}{2}\\log|\\Sigma^{-1}| -\\frac{1}{2}\\,\\mathrm{tr}\\left((N-1)S\\Sigma^{-1}\\right)$  \n\n$\\frac{\\partial}{\\partial\\Sigma^{-1}} \\left[-\\frac{N}{2}\\log|\\Sigma| -\\frac{1}{2}\\,\\mathrm{tr}\\left((N-1)S\\Sigma^{-1}\\right)\\right] = 0$  \n$\\hat\\Sigma_{MLE} = \\frac{N-1}{N} S$, which is biased.","metadata":{}},{"id":"c4aec95a-808d-4f8c-a62e-589069071bd2","cell_type":"markdown","source":"### Restricted Maximum Likelihood Estimator (REML)  \nThis estimator is designed to correct the bias\n\nThe idea of REML is to remove the effect of $\\mu$ before estimating $\\Sigma$. To motivate REML, consider the scalar case $Y_i \\sim N(\\mu,\\sigma^2), \\quad i=1,\\ldots,N$.  \n\nGoal: Find the MLE of $\\sigma^2$.  \n$\\log \\mathcal{L}(\\mu, \\sigma^2) \\propto -\\frac{N}{2}\\log(\\sigma^2) -\\frac{1}{2 \\sigma^2} \\sum_{i=1}^{N} (Y_i - \\mu)^2$  \n$\\frac{\\partial \\mathcal{L}(\\mu, \\sigma^2 | Y_1, \\cdots ,Y_N)}{\\partial \\sigma^{2}} = -\\frac{N}{2} \\frac{1}{\\sigma^2} +  \\frac{2}{\\sigma^4} \\sum_{i=1}^{N} (Y_i - \\mu)^2 = 0$  \n$\\hat{\\sigma}^2_{MLE} = \\frac{1}{N} \\sum_{i=1}^{N} (Y_i - \\mu)^2$  \n\nCheck: Is $\\hat{\\sigma}^2_{MLE}$ unbiased?  \n$E[\\hat{\\sigma}^2_{MLE}] = \\frac{1}{N} \\sum_{i=1}^{N}  E[(Y_i - \\mu)^2] = \\frac{1}{N} \\sum_{i=1}^{N} [\\text{Var}(Y_i - \\mu) + [E[Y_i - \\mu]]^2] = \\frac{1}{N} \\sum_{i=1}^{N} \\sigma^2 = \\frac{1}{N} N \\sigma^2 = \\sigma^2$  \nTherefore, $\\hat{\\sigma}^2_{MLE}$ is unbiased.  \n\nThus, $\\hat{\\sigma}^2_{MLE}$ is unbiased when $\\mu$ is known, and biased when $\\mu$ is unknown. The idea of REML is to find a linear transformation $Z$ of $Y$ that is free of $\\mu$. More specifically, we'll define $Z$ such that it has mean $0$.  \n\nREML in the univariate case: $Y_i \\sim N(\\mu,\\sigma^2), \\quad i=1,\\ldots,N$. Define $Z = Y_i - \\bar{Y}$.  ","metadata":{}},{"id":"54b84088-3ec5-4706-b3d2-b1efc4c4385d","cell_type":"markdown","source":"### REML Derivation\nREML in univariate setting:\n\nSuppose $Y_i \\sim N(\\mu,\\sigma^2)$, $i = 1,\\ldots,N$.  \n\nThen $\\bar Y \\sim N\\left(\\mu,\\frac{\\sigma^2}{N}\\right)$.  \n  \nDefine $Z_i = Y_i - \\bar Y$.\n\n#### Step 1: We want to find the distribution of $Z_i$\n\nWe know $Z_i$ is normally distributed based on results from Lecture 2. We need to find $E(Z_i)$ and $\\mathrm{Var}(Z_i)$.  \n  \nFirst, $E(Z_i) = E(Y_i - \\bar Y) = E(Y_i) - E(\\bar Y) = \\mu - \\mu = 0$.  \n  \nSecond, $\\mathrm{Var}(Z_i) = \\mathrm{Var}(Y_i - \\bar Y) = \\mathrm{Var}(Y_i) + \\mathrm{Var}(\\bar Y) - 2\\mathrm{Cov}(Y_i,\\bar Y)$  \n  \nNote: $\\mathrm{Cov}(Y_i,\\bar Y) = \\frac{1}{N}\\sum_{j=1}^N \\mathrm{Cov}(Y_i,Y_j)$  \n  \nSince $Y_i$ are iid, $\\mathrm{Cov}(Y_i,Y_j) = \\begin{cases} \\sigma^2 & i=j \\\\ 0 & i\\neq j \\end{cases}$.  \n  \nThus, $\\mathrm{Cov}(Y_i,\\bar Y) = \\frac{\\sigma^2}{N}$  \n  \nSo, $\\mathrm{Var}(Z_i)= \\sigma^2 + \\frac{\\sigma^2}{N} - 2\\frac{\\sigma^2}{N} = \\sigma^2\\left(\\frac{N-1}{N}\\right)$  \n  \nTherefore, $Z_i \\sim N\\left(0,\\sigma^2\\frac{N-1}{N}\\right)$.  \n\n#### Step 2: We want to find the MLE of $\\sigma^2$ based on $Z_1,\\ldots,Z_N$\n  \nTo derive the MLE, we write the log-likelihood of $Z_1,\\ldots,Z_N$.  \n  \nBut note:\n1. $Z_1,\\ldots,Z_N$ are not independent since $\\sum_{i=1}^N Z_i = 0$. This means $Z_N$ is fully determined by $Z_1,\\ldots,Z_{N-1}$. So the distribution of $Z_1,\\ldots,Z_N$ is determined by $Z_1,\\ldots,Z_{N-1}$.\n2. Observe that $\\mathrm{Cov}(Z_i,Z_j)= \\mathrm{Cov}(Y_i-\\bar Y, Y_j-\\bar Y) = \\mathrm{Cov}(Y_i,Y_j) - \\mathrm{Cov}(Y_i,\\bar Y) - \\mathrm{Cov}(\\bar Y,Y_j) + \\mathrm{Cov}(\\bar Y,\\bar Y)$. Thus, $ \\mathrm{Cov}(Z_i,Z_j) = \\begin{cases} \\sigma^2\\left(1-\\frac{1}{N}\\right) & i=j \\\\ -\\frac{\\sigma^2}{N} & i\\neq j \\end{cases} $  \n  \nLet $Z = (Z_1,\\ldots,Z_{N-1})^\\top$. Then the above result means $\\mathrm{Cov}(Z) = \\sigma^2 \\left( I_{N-1} - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^\\top \\right)$ where $\\mathbf{1}$ is an $(N-1)$-dimensional vector of 1â€™s.  \n  \nNow determine $\\det(\\mathrm{Cov}(Z))$.\n\nNote: If $A$ is invertible, $\\det(A+uv^\\top) = (1+v^\\top A^{-1}u)\\det(A) $.  \n\nLet $A = I_{N-1}$, $u = -\\frac{1}{N}\\mathbf{1}$, $v = \\mathbf{1}$.  \n  \nThen $ \\det(\\mathrm{Cov}(Z)) = (\\sigma^2)^{N-1}\\frac{1}{N}$.  \n\nNow find the inverse $(\\mathrm{Cov}(Z))^{-1}$  \n  \nNote: $(A+uv^\\top)^{-1} =A^{-1} - \\frac{A^{-1}uv^\\top A^{-1}}{1+v^\\top A^{-1}u}$.  \n  \nUsing this,\n\n$(\\mathrm{Cov}(Z))^{-1} = \\frac{1}{\\sigma^2} \\left(I_{N-1} + \\mathbf{1}\\mathbf{1}^\\top\\right)$\n\n\nWe now have all the pieces to form the log-likelihood of $Z_1,\\ldots,Z_{N-1}$.  \n$\\ell(\\sigma^2) = -\\frac12 \\log|\\mathrm{Cov}(Z)| -\\frac12 Z^\\top(\\mathrm{Cov}(Z))^{-1}Z $  \n\nNote: $Z^\\top Z = \\sum_{i=1}^{N-1} Z_i^2 $.   \n  \nSince $\\sum_{i=1}^N Z_i = 0$, we get $ \\ell(\\sigma^2) = -\\frac{N-1}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N Z_i^2 $.  \n  \nNow we can find the MLE of $\\sigma^2$.  \n  \nDifferentiate: $\\frac{\\partial \\ell}{\\partial \\sigma^2} = 0$\n\nThis gives $\\hat\\sigma^2_{\\text{REML}} = \\frac{1}{N-1} \\sum_{i=1}^N (Y_i-\\bar Y)^2$.  \n  \n### REML for Multivariate Data \n$ Y_i \\sim MVN(\\mu,\\Sigma)$  \n  \nDefine the matrix transformation $ Z = Y_i -\\bar{Y}$. We can show $ Z_i \\sim MVN(0, \\frac{N-1}{N}\\Sigma)$.","metadata":{}},{"id":"b66df4a6-1324-42a7-b166-6f3cd1261552","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}